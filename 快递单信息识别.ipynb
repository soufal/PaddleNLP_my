{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业\n",
    "\n",
    "更换数据集MSRA和ERNIE-Gram或BERT等预训练模型。\n",
    "\n",
    "- 数据集：\n",
    "`train_ds, test_ds = load_dataset(\"msra_ner\", splits=[\"train\", \"test\"])`\n",
    "- 模型：\n",
    "\t将`from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification`换成相应的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用PaddleNLP语义预训练模型ERNIE完成快递单信息抽取\n",
    "\n",
    "\n",
    "**注意**\n",
    "\n",
    "本项目代码需要使用GPU环境来运行:\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/767f625548714f03b105b6ccb3aa78df9080e38d329e445380f505ddec6c7042\" width=\"40%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "命名实体识别是NLP中一项非常基础的任务，是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。命名实体识别的准确度，决定了下游任务的效果，是NLP中的一个基础问题。在NER任务提供了两种解决方案，一类LSTM/GRU + CRF，通过RNN类的模型来抽取底层文本的信息，而CRF(条件随机场)模型来学习底层Token之间的联系；另外一类是通过预训练模型，例如ERNIE，BERT模型，直接来预测Token的标签信息。\n",
    "\n",
    "本项目将演示如何使用PaddleNLP语义预训练模型ERNIE完成从快递单中抽取姓名、电话、省、市、区、详细地址等内容，形成结构化信息。辅助物流行业从业者进行有效信息的提取，从而降低客户填单的成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在2017年之前，工业界和学术界对文本处理依赖于序列模型[Recurrent Neural Network (RNN)](https://baike.baidu.com/item/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/23199490?fromtitle=RNN&fromid=5707183&fr=aladdin).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width=\"40%\" height=\"30%\"> <br />\n",
    "</p><br><center>图1：RNN示意图</center></br>\n",
    "\n",
    "[基于BiGRU+CRF的快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)项目介绍了如何使用序列模型完成快递单信息抽取任务。\n",
    "<br>\n",
    "\n",
    "近年来随着深度学习的发展，模型参数的数量飞速增长。为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分NLP任务来说，构建大规模的标注数据集非常困难（成本过高），特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。为了利用这些数据，我们可以先从其中学习到一个好的表示，再将这些表示应用到其他任务中。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 在NLP任务上取得了很好的表现。\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的不断提高，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p><br><center>图2：预训练模型一览，图片来源于：https://github.com/thunlp/PLMpapers</center></br>\n",
    "                                                                                                                             \n",
    "本示例展示了以ERNIE([Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223))为代表的预训练模型如何Finetune完成序列标注任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "开源不易，希望大家多多支持~ \n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Requirement already up-to-date: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_a\tlabel\r\n",
      "黑\u0002龙\u0002江\u0002省\u0002双\u0002鸭\u0002山\u0002市\u0002尖\u0002山\u0002区\u0002八\u0002马\u0002路\u0002与\u0002东\u0002平\u0002行\u0002路\u0002交\u0002叉\u0002口\u0002北\u00024\u00020\u0002米\u0002韦\u0002业\u0002涛\u00021\u00028\u00026\u00020\u00020\u00020\u00020\u00029\u00021\u00027\u00022\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\r\n",
      "广\u0002西\u0002壮\u0002族\u0002自\u0002治\u0002区\u0002桂\u0002林\u0002市\u0002雁\u0002山\u0002区\u0002雁\u0002山\u0002镇\u0002西\u0002龙\u0002村\u0002老\u0002年\u0002活\u0002动\u0002中\u0002心\u00021\u00027\u00026\u00021\u00020\u00023\u00024\u00028\u00028\u00028\u00028\u0002羊\u0002卓\u0002卫\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "1\u00025\u00026\u00025\u00022\u00028\u00026\u00024\u00025\u00026\u00021\u0002河\u0002南\u0002省\u0002开\u0002封\u0002市\u0002顺\u0002河\u0002回\u0002族\u0002区\u0002顺\u0002河\u0002区\u0002公\u0002园\u0002路\u00023\u00022\u0002号\u0002赵\u0002本\u0002山\tT-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002A1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "河\u0002北\u0002省\u0002唐\u0002山\u0002市\u0002玉\u0002田\u0002县\u0002无\u0002终\u0002大\u0002街\u00021\u00025\u00029\u0002号\u00021\u00028\u00026\u00021\u00024\u00022\u00025\u00023\u00020\u00025\u00028\u0002尚\u0002汉\u0002生\tA1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n"
     ]
    }
   ],
   "source": [
    "# 下载并解压数据集\n",
    "from paddle.utils.download import get_path_from_url\n",
    "URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/waybill.tar.gz\"\n",
    "get_path_from_url(URL, \"./\")\n",
    "\n",
    "# 查看预测的数据\n",
    "!head -n 5 data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from utils import convert_example, evaluate, predict, load_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 加载自定义数据集\n",
    "\n",
    "推荐使用MapDataset()自定义数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)  # Skip header\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                yield words, labels\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# Create dataset, tokenizer and dataloader.\n",
    "train_ds, dev_ds, test_ds = load_dataset(datafiles=(\n",
    "        './data/train.txt', './data/dev.txt', './data/test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    }
   ],
   "source": [
    "# 训练集大小\r\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# 验证集大小\r\n",
    "print(len(dev_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# 测试集大小\r\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['1', '6', '6', '2', '0', '2', '0', '0', '0', '7', '7', '宣', '荣', '嗣', '甘', '肃', '省', '白', '银', '市', '会', '宁', '县', '河', '畔', '镇', '十', '字', '街', '金', '海', '超', '市', '西', '行', '5', '0', '米'], ['T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I', 'P-I', 'A1-B', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I'])\n",
      "(['1', '3', '5', '5', '2', '6', '6', '4', '3', '0', '7', '姜', '骏', '炜', '云', '南', '省', '德', '宏', '傣', '族', '景', '颇', '族', '自', '治', '州', '盈', '江', '县', '平', '原', '镇', '蜜', '回', '路', '下', '段'], ['T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I', 'P-I', 'A1-B', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I'])\n",
      "(['内', '蒙', '古', '自', '治', '区', '赤', '峰', '市', '阿', '鲁', '科', '尔', '沁', '旗', '汉', '林', '西', '街', '路', '南', '1', '3', '7', '0', '1', '0', '8', '5', '3', '9', '0', '那', '峥'], ['A1-B', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A3-I', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I'])\n",
      "(['广', '东', '省', '梅', '州', '市', '大', '埔', '县', '茶', '阳', '镇', '胜', '利', '路', '1', '3', '6', '0', '1', '3', '2', '8', '1', '7', '3', '张', '铱'], ['A1-B', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I'])\n",
      "(['新', '疆', '维', '吾', '尔', '自', '治', '区', '阿', '克', '苏', '地', '区', '阿', '克', '苏', '市', '步', '行', '街', '1', '0', '号', '1', '5', '8', '1', '0', '7', '8', '9', '3', '7', '8', '慕', '东', '霖'], ['A1-B', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A1-I', 'A2-B', 'A2-I', 'A2-I', 'A2-I', 'A2-I', 'A3-B', 'A3-I', 'A3-I', 'A3-I', 'A4-B', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'A4-I', 'T-B', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'T-I', 'P-B', 'P-I', 'P-I'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "每条数据包含一句文本和这个文本中每个汉字以及数字对应的label标签。\n",
    "\n",
    "之后，还需要对输入句子进行数据处理，如切词，映射词表id等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "<br><center>图3：ERNIE模型示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 09:35:53,773] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt\n",
      "100%|██████████| 90/90 [00:00<00:00, 4265.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 208, 515, 515, 249, 540, 249, 540, 540, 540, 589, 589, 803, 838, 2914, 1222, 1734, 244, 368, 797, 99, 32, 863, 308, 457, 2778, 484, 167, 436, 930, 192, 233, 634, 99, 213, 40, 317, 540, 256, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 40, [12, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 4, 5, 5, 6, 7, 7, 8, 9, 9, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12])\n"
     ]
    }
   ],
   "source": [
    "label_vocab = load_dict('./data/tag.dic')\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\n",
    "\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "test_ds.map(trans_func)\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读入\n",
    "\n",
    "使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "test_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PaddleNLP一键加载预训练模型\n",
    "\n",
    "\n",
    "快递单信息抽取本质是一个序列标注任务，PaddleNLP对于各种预训练模型已经内置了对于下游任务文本分类Fine-tune网络。以下教程以ERNIE为预训练模型完成序列标注任务。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification()`一行代码即可加载预训练模型ERNIE用于序列标注任务的fine-tune网络。其在ERNIE模型后拼接上一个全连接网络进行分类。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification.from_pretrained()`方法只需指定想要使用的模型名称和文本分类的类别数即可完成定义模型网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 09:36:05,177] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2021-06-12 09:36:05,179] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
      "100%|██████████| 392507/392507 [00:09<00:00, 39532.47it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "# Define the model netword and its loss\n",
    "model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。您可以使用PaddleNLP提供的模型，完成文本分类、序列标注、问答等任务。同时我们提供了众多预训练模型的参数权重供用户使用，其中包含了二十多种中文语言模型的预训练权重。中文的预训练模型有`bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, unified_transformer-12L-cn, unified_transformer-12L-cn-luge`等。\n",
    "\n",
    "更多预训练模型参考：[PaddleNLP Transformer API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)。\n",
    "\n",
    "更多预训练模型fine-tune下游任务使用方法，请参考：[examples](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设置Fine-Tune优化策略，模型配置\n",
    "适用于ERNIE/BERT这类Transformer模型的迁移优化学习率策略为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p><br><center>图4：动态学习率示意图</center></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:239: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.bool, the right dtype will convert to paddle.float32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 - step:1 - loss: 2.706165\n",
      "epoch:0 - step:2 - loss: 2.485035\n",
      "epoch:0 - step:3 - loss: 2.317535\n",
      "epoch:0 - step:4 - loss: 2.178066\n",
      "epoch:0 - step:5 - loss: 2.007460\n",
      "epoch:0 - step:6 - loss: 1.890076\n",
      "epoch:0 - step:7 - loss: 1.802366\n",
      "epoch:0 - step:8 - loss: 1.677067\n",
      "epoch:0 - step:9 - loss: 1.627942\n",
      "epoch:0 - step:10 - loss: 1.515404\n",
      "epoch:0 - step:11 - loss: 1.447197\n",
      "epoch:0 - step:12 - loss: 1.403117\n",
      "epoch:0 - step:13 - loss: 1.315436\n",
      "epoch:0 - step:14 - loss: 1.230516\n",
      "epoch:0 - step:15 - loss: 1.200576\n",
      "epoch:0 - step:16 - loss: 1.141350\n",
      "epoch:0 - step:17 - loss: 1.121737\n",
      "epoch:0 - step:18 - loss: 1.042802\n",
      "epoch:0 - step:19 - loss: 1.002271\n",
      "epoch:0 - step:20 - loss: 0.963703\n",
      "epoch:0 - step:21 - loss: 0.912682\n",
      "epoch:0 - step:22 - loss: 0.873136\n",
      "epoch:0 - step:23 - loss: 0.828306\n",
      "epoch:0 - step:24 - loss: 0.772590\n",
      "epoch:0 - step:25 - loss: 0.760018\n",
      "epoch:0 - step:26 - loss: 0.700998\n",
      "epoch:0 - step:27 - loss: 0.690887\n",
      "epoch:0 - step:28 - loss: 0.655108\n",
      "epoch:0 - step:29 - loss: 0.586785\n",
      "epoch:0 - step:30 - loss: 0.580484\n",
      "epoch:0 - step:31 - loss: 0.522591\n",
      "epoch:0 - step:32 - loss: 0.508540\n",
      "epoch:0 - step:33 - loss: 0.459742\n",
      "epoch:0 - step:34 - loss: 0.442098\n",
      "epoch:0 - step:35 - loss: 0.400869\n",
      "epoch:0 - step:36 - loss: 0.411278\n",
      "epoch:0 - step:37 - loss: 0.374442\n",
      "epoch:0 - step:38 - loss: 0.351367\n",
      "epoch:0 - step:39 - loss: 0.338283\n",
      "epoch:0 - step:40 - loss: 0.296886\n",
      "epoch:0 - step:41 - loss: 0.291387\n",
      "epoch:0 - step:42 - loss: 0.283996\n",
      "epoch:0 - step:43 - loss: 0.234360\n",
      "epoch:0 - step:44 - loss: 0.215246\n",
      "epoch:0 - step:45 - loss: 0.197447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 09:36:41,180] [ WARNING] - Compatibility Warning: The params of ChunkEvaluator.compute has been modified. The old version is `inputs`, `lengths`, `predictions`, `labels` while the current version is `lengths`, `predictions`, `labels`.  Please update the usage.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval precision: 0.934480 - recall: 0.959630 - f1: 0.946888\n",
      "epoch:1 - step:46 - loss: 0.200485\n",
      "epoch:1 - step:47 - loss: 0.197590\n",
      "epoch:1 - step:48 - loss: 0.187141\n",
      "epoch:1 - step:49 - loss: 0.169811\n",
      "epoch:1 - step:50 - loss: 0.172983\n",
      "epoch:1 - step:51 - loss: 0.163874\n",
      "epoch:1 - step:52 - loss: 0.138479\n",
      "epoch:1 - step:53 - loss: 0.116215\n",
      "epoch:1 - step:54 - loss: 0.112838\n",
      "epoch:1 - step:55 - loss: 0.126843\n",
      "epoch:1 - step:56 - loss: 0.093396\n",
      "epoch:1 - step:57 - loss: 0.112721\n",
      "epoch:1 - step:58 - loss: 0.085811\n",
      "epoch:1 - step:59 - loss: 0.087971\n",
      "epoch:1 - step:60 - loss: 0.126028\n",
      "epoch:1 - step:61 - loss: 0.100903\n",
      "epoch:1 - step:62 - loss: 0.091900\n",
      "epoch:1 - step:63 - loss: 0.058446\n",
      "epoch:1 - step:64 - loss: 0.088273\n",
      "epoch:1 - step:65 - loss: 0.092368\n",
      "epoch:1 - step:66 - loss: 0.061253\n",
      "epoch:1 - step:67 - loss: 0.077457\n",
      "epoch:1 - step:68 - loss: 0.073943\n",
      "epoch:1 - step:69 - loss: 0.043659\n",
      "epoch:1 - step:70 - loss: 0.066500\n",
      "epoch:1 - step:71 - loss: 0.069115\n",
      "epoch:1 - step:72 - loss: 0.061483\n",
      "epoch:1 - step:73 - loss: 0.070142\n",
      "epoch:1 - step:74 - loss: 0.056228\n",
      "epoch:1 - step:75 - loss: 0.044585\n",
      "epoch:1 - step:76 - loss: 0.042973\n",
      "epoch:1 - step:77 - loss: 0.038019\n",
      "epoch:1 - step:78 - loss: 0.050315\n",
      "epoch:1 - step:79 - loss: 0.043943\n",
      "epoch:1 - step:80 - loss: 0.035304\n",
      "epoch:1 - step:81 - loss: 0.035577\n",
      "epoch:1 - step:82 - loss: 0.057656\n",
      "epoch:1 - step:83 - loss: 0.047856\n",
      "epoch:1 - step:84 - loss: 0.039966\n",
      "epoch:1 - step:85 - loss: 0.038732\n",
      "epoch:1 - step:86 - loss: 0.045940\n",
      "epoch:1 - step:87 - loss: 0.045434\n",
      "epoch:1 - step:88 - loss: 0.026437\n",
      "epoch:1 - step:89 - loss: 0.035246\n",
      "epoch:1 - step:90 - loss: 0.027375\n",
      "eval precision: 0.978297 - recall: 0.985702 - f1: 0.981986\n",
      "epoch:2 - step:91 - loss: 0.028312\n",
      "epoch:2 - step:92 - loss: 0.049325\n",
      "epoch:2 - step:93 - loss: 0.030859\n",
      "epoch:2 - step:94 - loss: 0.020259\n",
      "epoch:2 - step:95 - loss: 0.028780\n",
      "epoch:2 - step:96 - loss: 0.037700\n",
      "epoch:2 - step:97 - loss: 0.040151\n",
      "epoch:2 - step:98 - loss: 0.027955\n",
      "epoch:2 - step:99 - loss: 0.022041\n",
      "epoch:2 - step:100 - loss: 0.017488\n",
      "epoch:2 - step:101 - loss: 0.038878\n",
      "epoch:2 - step:102 - loss: 0.023013\n",
      "epoch:2 - step:103 - loss: 0.019961\n",
      "epoch:2 - step:104 - loss: 0.014510\n",
      "epoch:2 - step:105 - loss: 0.036804\n",
      "epoch:2 - step:106 - loss: 0.047103\n",
      "epoch:2 - step:107 - loss: 0.019865\n",
      "epoch:2 - step:108 - loss: 0.013037\n",
      "epoch:2 - step:109 - loss: 0.047881\n",
      "epoch:2 - step:110 - loss: 0.046392\n",
      "epoch:2 - step:111 - loss: 0.016141\n",
      "epoch:2 - step:112 - loss: 0.030784\n",
      "epoch:2 - step:113 - loss: 0.019863\n",
      "epoch:2 - step:114 - loss: 0.015292\n",
      "epoch:2 - step:115 - loss: 0.030630\n",
      "epoch:2 - step:116 - loss: 0.030330\n",
      "epoch:2 - step:117 - loss: 0.014771\n",
      "epoch:2 - step:118 - loss: 0.031157\n",
      "epoch:2 - step:119 - loss: 0.022349\n",
      "epoch:2 - step:120 - loss: 0.011823\n",
      "epoch:2 - step:121 - loss: 0.009414\n",
      "epoch:2 - step:122 - loss: 0.012863\n",
      "epoch:2 - step:123 - loss: 0.040235\n",
      "epoch:2 - step:124 - loss: 0.014978\n",
      "epoch:2 - step:125 - loss: 0.016961\n",
      "epoch:2 - step:126 - loss: 0.012132\n",
      "epoch:2 - step:127 - loss: 0.037882\n",
      "epoch:2 - step:128 - loss: 0.021577\n",
      "epoch:2 - step:129 - loss: 0.012369\n",
      "epoch:2 - step:130 - loss: 0.012387\n",
      "epoch:2 - step:131 - loss: 0.032735\n",
      "epoch:2 - step:132 - loss: 0.033655\n",
      "epoch:2 - step:133 - loss: 0.009240\n",
      "epoch:2 - step:134 - loss: 0.012954\n",
      "epoch:2 - step:135 - loss: 0.054629\n",
      "eval precision: 0.986588 - recall: 0.989907 - f1: 0.988245\n",
      "epoch:3 - step:136 - loss: 0.015133\n",
      "epoch:3 - step:137 - loss: 0.010555\n",
      "epoch:3 - step:138 - loss: 0.010854\n",
      "epoch:3 - step:139 - loss: 0.008862\n",
      "epoch:3 - step:140 - loss: 0.011776\n",
      "epoch:3 - step:141 - loss: 0.013526\n",
      "epoch:3 - step:142 - loss: 0.029072\n",
      "epoch:3 - step:143 - loss: 0.011200\n",
      "epoch:3 - step:144 - loss: 0.009099\n",
      "epoch:3 - step:145 - loss: 0.009454\n",
      "epoch:3 - step:146 - loss: 0.013075\n",
      "epoch:3 - step:147 - loss: 0.011079\n",
      "epoch:3 - step:148 - loss: 0.006738\n",
      "epoch:3 - step:149 - loss: 0.007006\n",
      "epoch:3 - step:150 - loss: 0.026565\n",
      "epoch:3 - step:151 - loss: 0.009663\n",
      "epoch:3 - step:152 - loss: 0.009218\n",
      "epoch:3 - step:153 - loss: 0.006290\n",
      "epoch:3 - step:154 - loss: 0.023958\n",
      "epoch:3 - step:155 - loss: 0.040151\n",
      "epoch:3 - step:156 - loss: 0.016100\n",
      "epoch:3 - step:157 - loss: 0.025716\n",
      "epoch:3 - step:158 - loss: 0.011361\n",
      "epoch:3 - step:159 - loss: 0.007003\n",
      "epoch:3 - step:160 - loss: 0.021455\n",
      "epoch:3 - step:161 - loss: 0.025107\n",
      "epoch:3 - step:162 - loss: 0.006237\n",
      "epoch:3 - step:163 - loss: 0.010721\n",
      "epoch:3 - step:164 - loss: 0.010310\n",
      "epoch:3 - step:165 - loss: 0.008507\n",
      "epoch:3 - step:166 - loss: 0.005122\n",
      "epoch:3 - step:167 - loss: 0.006048\n",
      "epoch:3 - step:168 - loss: 0.037478\n",
      "epoch:3 - step:169 - loss: 0.016008\n",
      "epoch:3 - step:170 - loss: 0.008619\n",
      "epoch:3 - step:171 - loss: 0.005873\n",
      "epoch:3 - step:172 - loss: 0.026349\n",
      "epoch:3 - step:173 - loss: 0.015861\n",
      "epoch:3 - step:174 - loss: 0.007651\n",
      "epoch:3 - step:175 - loss: 0.013148\n",
      "epoch:3 - step:176 - loss: 0.009806\n",
      "epoch:3 - step:177 - loss: 0.012585\n",
      "epoch:3 - step:178 - loss: 0.007390\n",
      "epoch:3 - step:179 - loss: 0.006363\n",
      "epoch:3 - step:180 - loss: 0.005018\n",
      "eval precision: 0.989924 - recall: 0.991590 - f1: 0.990756\n",
      "epoch:4 - step:181 - loss: 0.005262\n",
      "epoch:4 - step:182 - loss: 0.005921\n",
      "epoch:4 - step:183 - loss: 0.006932\n",
      "epoch:4 - step:184 - loss: 0.004987\n",
      "epoch:4 - step:185 - loss: 0.005807\n",
      "epoch:4 - step:186 - loss: 0.016335\n",
      "epoch:4 - step:187 - loss: 0.012631\n",
      "epoch:4 - step:188 - loss: 0.004760\n",
      "epoch:4 - step:189 - loss: 0.005994\n",
      "epoch:4 - step:190 - loss: 0.006717\n",
      "epoch:4 - step:191 - loss: 0.012890\n",
      "epoch:4 - step:192 - loss: 0.005086\n",
      "epoch:4 - step:193 - loss: 0.004300\n",
      "epoch:4 - step:194 - loss: 0.010498\n",
      "epoch:4 - step:195 - loss: 0.026773\n",
      "epoch:4 - step:196 - loss: 0.008932\n",
      "epoch:4 - step:197 - loss: 0.007247\n",
      "epoch:4 - step:198 - loss: 0.004695\n",
      "epoch:4 - step:199 - loss: 0.029046\n",
      "epoch:4 - step:200 - loss: 0.030189\n",
      "epoch:4 - step:201 - loss: 0.010003\n",
      "epoch:4 - step:202 - loss: 0.014167\n",
      "epoch:4 - step:203 - loss: 0.006324\n",
      "epoch:4 - step:204 - loss: 0.004105\n",
      "epoch:4 - step:205 - loss: 0.008708\n",
      "epoch:4 - step:206 - loss: 0.011472\n",
      "epoch:4 - step:207 - loss: 0.005617\n",
      "epoch:4 - step:208 - loss: 0.006056\n",
      "epoch:4 - step:209 - loss: 0.004773\n",
      "epoch:4 - step:210 - loss: 0.009202\n",
      "epoch:4 - step:211 - loss: 0.003769\n",
      "epoch:4 - step:212 - loss: 0.003780\n",
      "epoch:4 - step:213 - loss: 0.026584\n",
      "epoch:4 - step:214 - loss: 0.004673\n",
      "epoch:4 - step:215 - loss: 0.003518\n",
      "epoch:4 - step:216 - loss: 0.011960\n",
      "epoch:4 - step:217 - loss: 0.024499\n",
      "epoch:4 - step:218 - loss: 0.006104\n",
      "epoch:4 - step:219 - loss: 0.005829\n",
      "epoch:4 - step:220 - loss: 0.010966\n",
      "epoch:4 - step:221 - loss: 0.007579\n",
      "epoch:4 - step:222 - loss: 0.030512\n",
      "epoch:4 - step:223 - loss: 0.005050\n",
      "epoch:4 - step:224 - loss: 0.004589\n",
      "epoch:4 - step:225 - loss: 0.003702\n",
      "eval precision: 0.990780 - recall: 0.994113 - f1: 0.992443\n",
      "epoch:5 - step:226 - loss: 0.007729\n",
      "epoch:5 - step:227 - loss: 0.008803\n",
      "epoch:5 - step:228 - loss: 0.009992\n",
      "epoch:5 - step:229 - loss: 0.004558\n",
      "epoch:5 - step:230 - loss: 0.003542\n",
      "epoch:5 - step:231 - loss: 0.003616\n",
      "epoch:5 - step:232 - loss: 0.006377\n",
      "epoch:5 - step:233 - loss: 0.003146\n",
      "epoch:5 - step:234 - loss: 0.004072\n",
      "epoch:5 - step:235 - loss: 0.003445\n",
      "epoch:5 - step:236 - loss: 0.020302\n",
      "epoch:5 - step:237 - loss: 0.002945\n",
      "epoch:5 - step:238 - loss: 0.002994\n",
      "epoch:5 - step:239 - loss: 0.002678\n",
      "epoch:5 - step:240 - loss: 0.008321\n",
      "epoch:5 - step:241 - loss: 0.017058\n",
      "epoch:5 - step:242 - loss: 0.003416\n",
      "epoch:5 - step:243 - loss: 0.002629\n",
      "epoch:5 - step:244 - loss: 0.019076\n",
      "epoch:5 - step:245 - loss: 0.035577\n",
      "epoch:5 - step:246 - loss: 0.003431\n",
      "epoch:5 - step:247 - loss: 0.008182\n",
      "epoch:5 - step:248 - loss: 0.003532\n",
      "epoch:5 - step:249 - loss: 0.003369\n",
      "epoch:5 - step:250 - loss: 0.010780\n",
      "epoch:5 - step:251 - loss: 0.004932\n",
      "epoch:5 - step:252 - loss: 0.005101\n",
      "epoch:5 - step:253 - loss: 0.007805\n",
      "epoch:5 - step:254 - loss: 0.006092\n",
      "epoch:5 - step:255 - loss: 0.003447\n",
      "epoch:5 - step:256 - loss: 0.009431\n",
      "epoch:5 - step:257 - loss: 0.004208\n",
      "epoch:5 - step:258 - loss: 0.026520\n",
      "epoch:5 - step:259 - loss: 0.002942\n",
      "epoch:5 - step:260 - loss: 0.002749\n",
      "epoch:5 - step:261 - loss: 0.003712\n",
      "epoch:5 - step:262 - loss: 0.010804\n",
      "epoch:5 - step:263 - loss: 0.006781\n",
      "epoch:5 - step:264 - loss: 0.002911\n",
      "epoch:5 - step:265 - loss: 0.006172\n",
      "epoch:5 - step:266 - loss: 0.004407\n",
      "epoch:5 - step:267 - loss: 0.014639\n",
      "epoch:5 - step:268 - loss: 0.003308\n",
      "epoch:5 - step:269 - loss: 0.014823\n",
      "epoch:5 - step:270 - loss: 0.003626\n",
      "eval precision: 0.989112 - recall: 0.993272 - f1: 0.991188\n",
      "epoch:6 - step:271 - loss: 0.004469\n",
      "epoch:6 - step:272 - loss: 0.002535\n",
      "epoch:6 - step:273 - loss: 0.004344\n",
      "epoch:6 - step:274 - loss: 0.003638\n",
      "epoch:6 - step:275 - loss: 0.014122\n",
      "epoch:6 - step:276 - loss: 0.016805\n",
      "epoch:6 - step:277 - loss: 0.008490\n",
      "epoch:6 - step:278 - loss: 0.004361\n",
      "epoch:6 - step:279 - loss: 0.004128\n",
      "epoch:6 - step:280 - loss: 0.004986\n",
      "epoch:6 - step:281 - loss: 0.009853\n",
      "epoch:6 - step:282 - loss: 0.003580\n",
      "epoch:6 - step:283 - loss: 0.002247\n",
      "epoch:6 - step:284 - loss: 0.004794\n",
      "epoch:6 - step:285 - loss: 0.009789\n",
      "epoch:6 - step:286 - loss: 0.003317\n",
      "epoch:6 - step:287 - loss: 0.003423\n",
      "epoch:6 - step:288 - loss: 0.002221\n",
      "epoch:6 - step:289 - loss: 0.004087\n",
      "epoch:6 - step:290 - loss: 0.023036\n",
      "epoch:6 - step:291 - loss: 0.002807\n",
      "epoch:6 - step:292 - loss: 0.004566\n",
      "epoch:6 - step:293 - loss: 0.003284\n",
      "epoch:6 - step:294 - loss: 0.002566\n",
      "epoch:6 - step:295 - loss: 0.005232\n",
      "epoch:6 - step:296 - loss: 0.009349\n",
      "epoch:6 - step:297 - loss: 0.006316\n",
      "epoch:6 - step:298 - loss: 0.005744\n",
      "epoch:6 - step:299 - loss: 0.002511\n",
      "epoch:6 - step:300 - loss: 0.002226\n",
      "epoch:6 - step:301 - loss: 0.003716\n",
      "epoch:6 - step:302 - loss: 0.002634\n",
      "epoch:6 - step:303 - loss: 0.022856\n",
      "epoch:6 - step:304 - loss: 0.009661\n",
      "epoch:6 - step:305 - loss: 0.001912\n",
      "epoch:6 - step:306 - loss: 0.002423\n",
      "epoch:6 - step:307 - loss: 0.005627\n",
      "epoch:6 - step:308 - loss: 0.006866\n",
      "epoch:6 - step:309 - loss: 0.002849\n",
      "epoch:6 - step:310 - loss: 0.001972\n",
      "epoch:6 - step:311 - loss: 0.002459\n",
      "epoch:6 - step:312 - loss: 0.004128\n",
      "epoch:6 - step:313 - loss: 0.003740\n",
      "epoch:6 - step:314 - loss: 0.002080\n",
      "epoch:6 - step:315 - loss: 0.002115\n",
      "eval precision: 0.992450 - recall: 0.994954 - f1: 0.993700\n",
      "epoch:7 - step:316 - loss: 0.002007\n",
      "epoch:7 - step:317 - loss: 0.002385\n",
      "epoch:7 - step:318 - loss: 0.006129\n",
      "epoch:7 - step:319 - loss: 0.002102\n",
      "epoch:7 - step:320 - loss: 0.001897\n",
      "epoch:7 - step:321 - loss: 0.001989\n",
      "epoch:7 - step:322 - loss: 0.002748\n",
      "epoch:7 - step:323 - loss: 0.001922\n",
      "epoch:7 - step:324 - loss: 0.003026\n",
      "epoch:7 - step:325 - loss: 0.002680\n",
      "epoch:7 - step:326 - loss: 0.001980\n",
      "epoch:7 - step:327 - loss: 0.001920\n",
      "epoch:7 - step:328 - loss: 0.001617\n",
      "epoch:7 - step:329 - loss: 0.001800\n",
      "epoch:7 - step:330 - loss: 0.004862\n",
      "epoch:7 - step:331 - loss: 0.001874\n",
      "epoch:7 - step:332 - loss: 0.001820\n",
      "epoch:7 - step:333 - loss: 0.003147\n",
      "epoch:7 - step:334 - loss: 0.024397\n",
      "epoch:7 - step:335 - loss: 0.025947\n",
      "epoch:7 - step:336 - loss: 0.003553\n",
      "epoch:7 - step:337 - loss: 0.003092\n",
      "epoch:7 - step:338 - loss: 0.004201\n",
      "epoch:7 - step:339 - loss: 0.001590\n",
      "epoch:7 - step:340 - loss: 0.005065\n",
      "epoch:7 - step:341 - loss: 0.008217\n",
      "epoch:7 - step:342 - loss: 0.001841\n",
      "epoch:7 - step:343 - loss: 0.003292\n",
      "epoch:7 - step:344 - loss: 0.001976\n",
      "epoch:7 - step:345 - loss: 0.004097\n",
      "epoch:7 - step:346 - loss: 0.003616\n",
      "epoch:7 - step:347 - loss: 0.002521\n",
      "epoch:7 - step:348 - loss: 0.028181\n",
      "epoch:7 - step:349 - loss: 0.001766\n",
      "epoch:7 - step:350 - loss: 0.001482\n",
      "epoch:7 - step:351 - loss: 0.001689\n",
      "epoch:7 - step:352 - loss: 0.003600\n",
      "epoch:7 - step:353 - loss: 0.002087\n",
      "epoch:7 - step:354 - loss: 0.002158\n",
      "epoch:7 - step:355 - loss: 0.001659\n",
      "epoch:7 - step:356 - loss: 0.006893\n",
      "epoch:7 - step:357 - loss: 0.002137\n",
      "epoch:7 - step:358 - loss: 0.001482\n",
      "epoch:7 - step:359 - loss: 0.002342\n",
      "epoch:7 - step:360 - loss: 0.001769\n",
      "eval precision: 0.991604 - recall: 0.993272 - f1: 0.992437\n",
      "epoch:8 - step:361 - loss: 0.001632\n",
      "epoch:8 - step:362 - loss: 0.001714\n",
      "epoch:8 - step:363 - loss: 0.001924\n",
      "epoch:8 - step:364 - loss: 0.002141\n",
      "epoch:8 - step:365 - loss: 0.001475\n",
      "epoch:8 - step:366 - loss: 0.002080\n",
      "epoch:8 - step:367 - loss: 0.003147\n",
      "epoch:8 - step:368 - loss: 0.001375\n",
      "epoch:8 - step:369 - loss: 0.002500\n",
      "epoch:8 - step:370 - loss: 0.001659\n",
      "epoch:8 - step:371 - loss: 0.004948\n",
      "epoch:8 - step:372 - loss: 0.002417\n",
      "epoch:8 - step:373 - loss: 0.005268\n",
      "epoch:8 - step:374 - loss: 0.002435\n",
      "epoch:8 - step:375 - loss: 0.003549\n",
      "epoch:8 - step:376 - loss: 0.001566\n",
      "epoch:8 - step:377 - loss: 0.001545\n",
      "epoch:8 - step:378 - loss: 0.005652\n",
      "epoch:8 - step:379 - loss: 0.001830\n",
      "epoch:8 - step:380 - loss: 0.021830\n",
      "epoch:8 - step:381 - loss: 0.001396\n",
      "epoch:8 - step:382 - loss: 0.004671\n",
      "epoch:8 - step:383 - loss: 0.003024\n",
      "epoch:8 - step:384 - loss: 0.002513\n",
      "epoch:8 - step:385 - loss: 0.001994\n",
      "epoch:8 - step:386 - loss: 0.002224\n",
      "epoch:8 - step:387 - loss: 0.001622\n",
      "epoch:8 - step:388 - loss: 0.002879\n",
      "epoch:8 - step:389 - loss: 0.003897\n",
      "epoch:8 - step:390 - loss: 0.002455\n",
      "epoch:8 - step:391 - loss: 0.001383\n",
      "epoch:8 - step:392 - loss: 0.001496\n",
      "epoch:8 - step:393 - loss: 0.022415\n",
      "epoch:8 - step:394 - loss: 0.001397\n",
      "epoch:8 - step:395 - loss: 0.001227\n",
      "epoch:8 - step:396 - loss: 0.001708\n",
      "epoch:8 - step:397 - loss: 0.007012\n",
      "epoch:8 - step:398 - loss: 0.006426\n",
      "epoch:8 - step:399 - loss: 0.001491\n",
      "epoch:8 - step:400 - loss: 0.001616\n",
      "epoch:8 - step:401 - loss: 0.002138\n",
      "epoch:8 - step:402 - loss: 0.001940\n",
      "epoch:8 - step:403 - loss: 0.001390\n",
      "epoch:8 - step:404 - loss: 0.001494\n",
      "epoch:8 - step:405 - loss: 0.001453\n",
      "eval precision: 0.996639 - recall: 0.997477 - f1: 0.997058\n",
      "epoch:9 - step:406 - loss: 0.001270\n",
      "epoch:9 - step:407 - loss: 0.001369\n",
      "epoch:9 - step:408 - loss: 0.001337\n",
      "epoch:9 - step:409 - loss: 0.001233\n",
      "epoch:9 - step:410 - loss: 0.002024\n",
      "epoch:9 - step:411 - loss: 0.001681\n",
      "epoch:9 - step:412 - loss: 0.002479\n",
      "epoch:9 - step:413 - loss: 0.001279\n",
      "epoch:9 - step:414 - loss: 0.001292\n",
      "epoch:9 - step:415 - loss: 0.001945\n",
      "epoch:9 - step:416 - loss: 0.001267\n",
      "epoch:9 - step:417 - loss: 0.001324\n",
      "epoch:9 - step:418 - loss: 0.001129\n",
      "epoch:9 - step:419 - loss: 0.001186\n",
      "epoch:9 - step:420 - loss: 0.004528\n",
      "epoch:9 - step:421 - loss: 0.001429\n",
      "epoch:9 - step:422 - loss: 0.001279\n",
      "epoch:9 - step:423 - loss: 0.004867\n",
      "epoch:9 - step:424 - loss: 0.001846\n",
      "epoch:9 - step:425 - loss: 0.025143\n",
      "epoch:9 - step:426 - loss: 0.001346\n",
      "epoch:9 - step:427 - loss: 0.002151\n",
      "epoch:9 - step:428 - loss: 0.001947\n",
      "epoch:9 - step:429 - loss: 0.001124\n",
      "epoch:9 - step:430 - loss: 0.004595\n",
      "epoch:9 - step:431 - loss: 0.001190\n",
      "epoch:9 - step:432 - loss: 0.001121\n",
      "epoch:9 - step:433 - loss: 0.002722\n",
      "epoch:9 - step:434 - loss: 0.001539\n",
      "epoch:9 - step:435 - loss: 0.002112\n",
      "epoch:9 - step:436 - loss: 0.001156\n",
      "epoch:9 - step:437 - loss: 0.001664\n",
      "epoch:9 - step:438 - loss: 0.017925\n",
      "epoch:9 - step:439 - loss: 0.001216\n",
      "epoch:9 - step:440 - loss: 0.001132\n",
      "epoch:9 - step:441 - loss: 0.001113\n",
      "epoch:9 - step:442 - loss: 0.005598\n",
      "epoch:9 - step:443 - loss: 0.002759\n",
      "epoch:9 - step:444 - loss: 0.001784\n",
      "epoch:9 - step:445 - loss: 0.001314\n",
      "epoch:9 - step:446 - loss: 0.001103\n",
      "epoch:9 - step:447 - loss: 0.001320\n",
      "epoch:9 - step:448 - loss: 0.002703\n",
      "epoch:9 - step:449 - loss: 0.001146\n",
      "epoch:9 - step:450 - loss: 0.001299\n",
      "eval precision: 0.994958 - recall: 0.995795 - f1: 0.995376\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(10):\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\n",
    "    evaluate(model, metric, dev_loader)\n",
    "\n",
    "    paddle.save(model.state_dict(),\n",
    "                './ernie_result/model_%d.pdparams' % step)\n",
    "# model.save_pretrained('./checkpoint')\n",
    "# tokenizer.save_pretrained('./checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型预测\n",
    "\n",
    "训练保存好的模型，即可用于预测。如以下示例代码自定义预测数据，调用`predict()`函数即可一键预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: ernie_results.txt, some examples are shown below: \n",
      "('黑龙江省', 'A1')('双鸭山市', 'A2')('尖山区', 'A3')('八马路与东平行路交叉口北40米', 'A4')('韦业涛', 'P')('18600009172', 'T')\n",
      "('广西壮族自治区', 'A1')('桂林市', 'A2')('雁山区', 'A3')('雁山镇西龙村老年活动中心', 'A4')('17610348888', 'T')('羊卓卫', 'P')\n",
      "('15652864561', 'T')('河南省', 'A1')('开封市', 'A2')('顺河回族区', 'A3')('顺河区公园路32号', 'A4')('赵本山', 'P')\n",
      "('河北省', 'A1')('唐山市', 'A2')('玉田县', 'A3')('无终大街159号', 'A4')('18614253058', 'T')('尚汉生', 'P')\n",
      "('台湾', 'A1')('台中市', 'A2')('北区', 'A3')('北区锦新街18号', 'A4')('18511226708', 'T')('蓟丽', 'P')\n",
      "('廖梓琪', 'P')('18514743222', 'T')('湖北省', 'A1')('宜昌市', 'A2')('长阳土家族自治县', 'A3')('贺家坪镇贺家坪村一组临河1号', 'A4')\n",
      "('江苏省', 'A1')('南通市', 'A2')('海门市', 'A3')('孝威村孝威路88号', 'A4')('18611840623', 'T')('计星仪', 'P')\n",
      "('17601674746', 'T')('赵春丽', 'P')('内蒙古自治区', 'A1')('乌兰察布市', 'A2')('凉城县', 'A3')('新建街', 'A4')\n",
      "('云南省', 'A1')('临沧市', 'A2')('耿马傣族佤族自治县', 'A3')('鑫源路法院对面', 'A4')('许贞爱', 'P')('18510566685', 'T')\n",
      "('四川省', 'A1')('成都市', 'A2')('双流区', 'A3')('东升镇北仓路196号', 'A4')('耿丕岭', 'P')('18513466161', 'T')\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader, test_ds, label_vocab)\n",
    "file_path = \"ernie_results.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write(\"\\n\".join(preds))\n",
    "# Print some examples\n",
    "print(\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\n",
    "    % file_path)\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 进一步使用CRF\n",
    "\n",
    "PaddleNLP提供了CRF Layer，它能够学习label之间的关系，能够帮助模型更好地学习、预测序列标注任务。\n",
    "\n",
    "我们在PaddleNLP仓库中提供了[示例](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/information_extraction/waybill_ie/run_ernie_crf.py)，您可以参照示例代码使用Ernie-CRF结构完成快递单信息抽取任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\r\n",
    "from paddlenlp.transformers import ErniePretrainedModel\r\n",
    "from paddlenlp.layers.crf import LinearChainCrf, ViterbiDecoder, LinearChainCrfLoss\r\n",
    "\r\n",
    "class ErnieCrfForTokenClassification(nn.Layer):\r\n",
    "    def __init__(self, ernie, crf_lr=100):\r\n",
    "        super().__init__()\r\n",
    "        self.num_classes = ernie.num_classes\r\n",
    "        self.ernie = ernie  # allow ernie to be config\r\n",
    "        self.crf = LinearChainCrf(\r\n",
    "            self.num_classes, crf_lr=crf_lr, with_start_stop_tag=False)\r\n",
    "        self.crf_loss = LinearChainCrfLoss(self.crf)\r\n",
    "        self.viterbi_decoder = ViterbiDecoder(\r\n",
    "            self.crf.transitions, with_start_stop_tag=False)\r\n",
    "\r\n",
    "    def forward(self,\r\n",
    "                input_ids,\r\n",
    "                token_type_ids=None,\r\n",
    "                position_ids=None,\r\n",
    "                attention_mask=None,\r\n",
    "                lengths=None,\r\n",
    "                labels=None):\r\n",
    "        logits = self.ernie(\r\n",
    "            input_ids,\r\n",
    "            token_type_ids=token_type_ids,\r\n",
    "            attention_mask=attention_mask,\r\n",
    "            position_ids=position_ids)\r\n",
    "\r\n",
    "        if labels is not None:\r\n",
    "            loss = self.crf_loss(logits, lengths, labels)\r\n",
    "            return loss\r\n",
    "        else:\r\n",
    "            _, prediction = self.viterbi_decoder(logits, lengths)\r\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 09:41:51,450] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 - step:1 - loss: 105.995605\n",
      "epoch:0 - step:2 - loss: 99.970711\n",
      "epoch:0 - step:3 - loss: 89.679108\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "(External)  Cublas error, `CUBLAS_STATUS_EXECUTION_FAILED`. The GPU program failed to execute. This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons.  (at /paddle/paddle/fluid/operators/math/blas_impl.cu.h:40)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-095211606ede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             input_ids, token_type_ids, lengths=length, labels=labels)\n\u001b[1;32m     23\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mavg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-247>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    224\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 core.dygraph_run_backward([self], [grad_tensor], retain_graph,\n\u001b[0;32m--> 236\u001b[0;31m                                           framework._dygraph_tracer())\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mOSError\u001b[0m: (External)  Cublas error, `CUBLAS_STATUS_EXECUTION_FAILED`. The GPU program failed to execute. This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons.  (at /paddle/paddle/fluid/operators/math/blas_impl.cu.h:40)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "使用Ernie-CRF结构完成快递单信息抽取任务\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# 定义Ernie-CRF模型结构\r\n",
    "ernie = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))\r\n",
    "model = ErnieCrfForTokenClassification(ernie)\r\n",
    "\r\n",
    "# 优化策略、模型参数配置\r\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\r\n",
    "# loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\r\n",
    "\r\n",
    "# 模型训练\r\n",
    "step = 0\r\n",
    "for epoch in range(10):\r\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\r\n",
    "\r\n",
    "        loss = model(\r\n",
    "            input_ids, token_type_ids, lengths=length, labels=labels)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        avg_loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "        step += 1\r\n",
    "\r\n",
    "        # print(epoch)\r\n",
    "        # print(step)\r\n",
    "        # print(loss)\r\n",
    "        print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, avg_loss))\r\n",
    "    evaluate(model, metric, dev_loader)\r\n",
    "\r\n",
    "    paddle.save(model.state_dict(),\r\n",
    "                './ernie_crf_result/model_%d.pdparams' % step)\r\n",
    "# model.save_pretrained('./checkpoint')\r\n",
    "# tokenizer.save_pretrained('./checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 作业部分\n",
    "\n",
    "更换数据集MSRA和ERNIE-Gram或BERT等预训练模型。\n",
    "\n",
    "- 数据集：\n",
    "`train_ds, test_ds = load_dataset(\"msra_ner\", splits=[\"train\", \"test\"])`\n",
    "- 模型：\n",
    "\t将`from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification`换成相应的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "import paddle\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from paddlenlp.transformers import BertTokenizer, BertForTokenClassification\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "\r\n",
    "\r\n",
    "from utils_homework import convert_example, evaluate, predict, load_dict\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "\r\n",
    "# 由于MSRA_NER数据集没有dev dataset，我们这里重复加载test dataset作为dev_ds\r\n",
    "train_ds, dev_ds, test_ds = load_dataset(\r\n",
    "        'msra_ner', splits=('train', 'test', 'test'), lazy=False)\r\n",
    "\r\n",
    "# 注意删除 label_vocab = load_dict('./data/tag.dic')\r\n",
    "label_vocab = {label:label_id for label_id, label in enumerate(train_ds.label_list)}\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3442\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 10:56:00,752] [    INFO] - Found /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 52, [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# 使用bert模型\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\r\n",
    "\r\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\r\n",
    "\r\n",
    "train_ds.map(trans_func)\r\n",
    "dev_ds.map(trans_func)\r\n",
    "test_ds.map(trans_func)\r\n",
    "print (train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle.io import DistributedBatchSampler\r\n",
    "from paddle.io import BatchSampler\r\n",
    "\r\n",
    "ignore_label = -1\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(),  # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "train_batch = DistributedBatchSampler(\r\n",
    "    dataset=train_ds, \r\n",
    "    batch_size=32, \r\n",
    "    shuffle=True\r\n",
    ")\r\n",
    "\r\n",
    "train_loader = paddle.io.DataLoader(\r\n",
    "    dataset=train_ds,\r\n",
    "    batch_sampler=train_batch,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)\r\n",
    "\r\n",
    "dev_batch = BatchSampler(\r\n",
    "    dataset=dev_ds,\r\n",
    "    batch_size=32,\r\n",
    "    shuffle=False\r\n",
    ")\r\n",
    "dev_loader = paddle.io.DataLoader(\r\n",
    "    dataset=dev_ds,\r\n",
    "    batch_sampler=dev_batch,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)\r\n",
    "\r\n",
    "test_batch = BatchSampler(\r\n",
    "    dataset=test_ds,\r\n",
    "    batch_size=32,\r\n",
    "    shuffle=False\r\n",
    ")\r\n",
    "test_loader = paddle.io.DataLoader(\r\n",
    "    dataset=test_ds,\r\n",
    "    batch_sampler=test_batch,\r\n",
    "    return_list=True,\r\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 10:56:13,471] [    INFO] - Downloading http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-base-chinese\n",
      "[2021-06-12 10:56:13,545] [    INFO] - Downloading bert-base-chinese.pdparams from http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams\n",
      "100%|██████████| 696494/696494 [00:15<00:00, 43843.24it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\r\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_classes=len(label_vocab))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle.nn import CrossEntropyLoss\r\n",
    "from paddle.optimizer import AdamW\r\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:239: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.bool, the right dtype will convert to paddle.float32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "[2021-06-12 10:57:05,930] [ WARNING] - Compatibility Warning: The params of ChunkEvaluator.compute has been modified. The old version is `inputs`, `lengths`, `predictions`, `labels` while the current version is `lengths`, `predictions`, `labels`.  Please update the usage.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 10, epoch: 0, batch: 9, loss: 0.47951, precission: 0.00410, recall: 0.01099, f1_score: 0.00597, speed: 4.75 step/s\n",
      "global_step: 20, epoch: 0, batch: 19, loss: 0.36871, precission: 0.00409, recall: 0.00589, f1_score: 0.00483, speed: 4.83 step/s\n",
      "global_step: 30, epoch: 0, batch: 29, loss: 0.37598, precission: 0.00663, recall: 0.00720, f1_score: 0.00691, speed: 4.99 step/s\n",
      "global_step: 40, epoch: 0, batch: 39, loss: 0.23423, precission: 0.05546, recall: 0.06139, f1_score: 0.05827, speed: 4.77 step/s\n",
      "global_step: 50, epoch: 0, batch: 49, loss: 0.12584, precission: 0.12685, recall: 0.14440, f1_score: 0.13506, speed: 5.05 step/s\n",
      "global_step: 60, epoch: 0, batch: 59, loss: 0.12131, precission: 0.20650, recall: 0.23506, f1_score: 0.21985, speed: 4.93 step/s\n",
      "global_step: 70, epoch: 0, batch: 69, loss: 0.05611, precission: 0.26574, recall: 0.30520, f1_score: 0.28411, speed: 4.84 step/s\n",
      "global_step: 80, epoch: 0, batch: 79, loss: 0.05956, precission: 0.31416, recall: 0.35918, f1_score: 0.33516, speed: 5.00 step/s\n",
      "global_step: 90, epoch: 0, batch: 89, loss: 0.07515, precission: 0.35395, recall: 0.40375, f1_score: 0.37721, speed: 4.85 step/s\n",
      "global_step: 100, epoch: 0, batch: 99, loss: 0.06284, precission: 0.39003, recall: 0.44283, f1_score: 0.41476, speed: 5.07 step/s\n",
      "eval dev loss: 0.05323, precission: 0.77925, recall: 0.83888, f1_score: 0.80797\n",
      "save model at global step : 100, best_precission: 0.77925, best_recall: 0.00000, best val f1_score: 0.80797\n",
      "global_step: 110, epoch: 0, batch: 109, loss: 0.06722, precission: 0.77512, recall: 0.83616, f1_score: 0.80448, speed: 0.65 step/s\n",
      "global_step: 120, epoch: 0, batch: 119, loss: 0.04624, precission: 0.76800, recall: 0.83098, f1_score: 0.79825, speed: 4.74 step/s\n",
      "global_step: 130, epoch: 0, batch: 129, loss: 0.03938, precission: 0.76870, recall: 0.83155, f1_score: 0.79889, speed: 5.23 step/s\n",
      "global_step: 140, epoch: 0, batch: 139, loss: 0.10679, precission: 0.76818, recall: 0.82945, f1_score: 0.79764, speed: 4.68 step/s\n",
      "global_step: 150, epoch: 0, batch: 149, loss: 0.08538, precission: 0.76790, recall: 0.82795, f1_score: 0.79680, speed: 4.96 step/s\n",
      "global_step: 160, epoch: 0, batch: 159, loss: 0.04782, precission: 0.76809, recall: 0.82759, f1_score: 0.79673, speed: 4.84 step/s\n",
      "global_step: 170, epoch: 0, batch: 169, loss: 0.13947, precission: 0.76965, recall: 0.82890, f1_score: 0.79818, speed: 5.09 step/s\n",
      "global_step: 180, epoch: 0, batch: 179, loss: 0.05281, precission: 0.77146, recall: 0.83016, f1_score: 0.79974, speed: 4.85 step/s\n",
      "global_step: 190, epoch: 0, batch: 189, loss: 0.02264, precission: 0.77485, recall: 0.83247, f1_score: 0.80263, speed: 4.53 step/s\n",
      "global_step: 200, epoch: 0, batch: 199, loss: 0.04727, precission: 0.77653, recall: 0.83414, f1_score: 0.80431, speed: 5.18 step/s\n",
      "eval dev loss: 0.03589, precission: 0.82926, recall: 0.88249, f1_score: 0.85504\n",
      "save model at global step : 200, best_precission: 0.82926, best_recall: 0.00000, best val f1_score: 0.85504\n",
      "global_step: 210, epoch: 0, batch: 209, loss: 0.04213, precission: 0.82857, recall: 0.87974, f1_score: 0.85339, speed: 0.68 step/s\n",
      "global_step: 220, epoch: 0, batch: 219, loss: 0.05426, precission: 0.82331, recall: 0.87641, f1_score: 0.84903, speed: 5.04 step/s\n",
      "global_step: 230, epoch: 0, batch: 229, loss: 0.09643, precission: 0.82726, recall: 0.87751, f1_score: 0.85164, speed: 4.78 step/s\n",
      "global_step: 240, epoch: 0, batch: 239, loss: 0.01662, precission: 0.82633, recall: 0.87699, f1_score: 0.85091, speed: 4.78 step/s\n",
      "global_step: 250, epoch: 0, batch: 249, loss: 0.03368, precission: 0.82626, recall: 0.87698, f1_score: 0.85086, speed: 4.85 step/s\n",
      "global_step: 260, epoch: 0, batch: 259, loss: 0.01278, precission: 0.82535, recall: 0.87669, f1_score: 0.85025, speed: 4.96 step/s\n",
      "global_step: 270, epoch: 0, batch: 269, loss: 0.03346, precission: 0.82457, recall: 0.87446, f1_score: 0.84878, speed: 4.61 step/s\n",
      "global_step: 280, epoch: 0, batch: 279, loss: 0.02195, precission: 0.82468, recall: 0.87466, f1_score: 0.84893, speed: 4.81 step/s\n",
      "global_step: 290, epoch: 0, batch: 289, loss: 0.02316, precission: 0.82562, recall: 0.87561, f1_score: 0.84988, speed: 4.98 step/s\n",
      "global_step: 300, epoch: 0, batch: 299, loss: 0.03244, precission: 0.82813, recall: 0.87662, f1_score: 0.85169, speed: 4.81 step/s\n",
      "eval dev loss: 0.03351, precission: 0.85686, recall: 0.89390, f1_score: 0.87499\n",
      "save model at global step : 300, best_precission: 0.85686, best_recall: 0.00000, best val f1_score: 0.87499\n",
      "global_step: 310, epoch: 0, batch: 309, loss: 0.04631, precission: 0.85608, recall: 0.89348, f1_score: 0.87438, speed: 0.67 step/s\n",
      "global_step: 320, epoch: 0, batch: 319, loss: 0.03752, precission: 0.85159, recall: 0.88991, f1_score: 0.87033, speed: 4.79 step/s\n",
      "global_step: 330, epoch: 0, batch: 329, loss: 0.05436, precission: 0.84750, recall: 0.88727, f1_score: 0.86693, speed: 4.93 step/s\n",
      "global_step: 340, epoch: 0, batch: 339, loss: 0.04643, precission: 0.84728, recall: 0.88710, f1_score: 0.86673, speed: 4.59 step/s\n",
      "global_step: 350, epoch: 0, batch: 349, loss: 0.04557, precission: 0.84817, recall: 0.88741, f1_score: 0.86735, speed: 4.62 step/s\n",
      "global_step: 360, epoch: 0, batch: 359, loss: 0.04378, precission: 0.84781, recall: 0.88793, f1_score: 0.86740, speed: 4.65 step/s\n",
      "global_step: 370, epoch: 0, batch: 369, loss: 0.01840, precission: 0.84709, recall: 0.88806, f1_score: 0.86709, speed: 5.11 step/s\n",
      "global_step: 380, epoch: 0, batch: 379, loss: 0.02481, precission: 0.84841, recall: 0.88734, f1_score: 0.86744, speed: 4.92 step/s\n",
      "global_step: 390, epoch: 0, batch: 389, loss: 0.02625, precission: 0.84938, recall: 0.88874, f1_score: 0.86861, speed: 4.83 step/s\n",
      "global_step: 400, epoch: 0, batch: 399, loss: 0.00978, precission: 0.85179, recall: 0.89035, f1_score: 0.87065, speed: 4.60 step/s\n",
      "eval dev loss: 0.03201, precission: 0.86841, recall: 0.90644, f1_score: 0.88702\n",
      "save model at global step : 400, best_precission: 0.86841, best_recall: 0.00000, best val f1_score: 0.88702\n",
      "global_step: 410, epoch: 0, batch: 409, loss: 0.03158, precission: 0.86410, recall: 0.90278, f1_score: 0.88302, speed: 0.67 step/s\n",
      "global_step: 420, epoch: 0, batch: 419, loss: 0.03350, precission: 0.86574, recall: 0.90244, f1_score: 0.88371, speed: 4.61 step/s\n",
      "global_step: 430, epoch: 0, batch: 429, loss: 0.03406, precission: 0.86271, recall: 0.90212, f1_score: 0.88197, speed: 4.72 step/s\n",
      "global_step: 440, epoch: 0, batch: 439, loss: 0.01944, precission: 0.86270, recall: 0.90177, f1_score: 0.88180, speed: 4.58 step/s\n",
      "global_step: 450, epoch: 0, batch: 449, loss: 0.02434, precission: 0.86259, recall: 0.90191, f1_score: 0.88181, speed: 4.71 step/s\n",
      "global_step: 460, epoch: 0, batch: 459, loss: 0.03030, precission: 0.86145, recall: 0.90165, f1_score: 0.88109, speed: 4.73 step/s\n",
      "global_step: 470, epoch: 0, batch: 469, loss: 0.01944, precission: 0.86079, recall: 0.90079, f1_score: 0.88033, speed: 4.66 step/s\n",
      "global_step: 480, epoch: 0, batch: 479, loss: 0.02203, precission: 0.85988, recall: 0.89999, f1_score: 0.87948, speed: 5.05 step/s\n",
      "global_step: 490, epoch: 0, batch: 489, loss: 0.03184, precission: 0.86009, recall: 0.90027, f1_score: 0.87972, speed: 4.69 step/s\n",
      "global_step: 500, epoch: 0, batch: 499, loss: 0.01861, precission: 0.86014, recall: 0.89883, f1_score: 0.87906, speed: 4.81 step/s\n",
      "eval dev loss: 0.02857, precission: 0.89244, recall: 0.92066, f1_score: 0.90633\n",
      "save model at global step : 500, best_precission: 0.89244, best_recall: 0.00000, best val f1_score: 0.90633\n",
      "global_step: 510, epoch: 0, batch: 509, loss: 0.04081, precission: 0.89112, recall: 0.91987, f1_score: 0.90527, speed: 0.67 step/s\n",
      "global_step: 520, epoch: 0, batch: 519, loss: 0.02649, precission: 0.88924, recall: 0.91810, f1_score: 0.90344, speed: 4.85 step/s\n",
      "global_step: 530, epoch: 0, batch: 529, loss: 0.02596, precission: 0.88573, recall: 0.91670, f1_score: 0.90095, speed: 5.10 step/s\n",
      "global_step: 540, epoch: 0, batch: 539, loss: 0.02944, precission: 0.88403, recall: 0.91512, f1_score: 0.89930, speed: 4.85 step/s\n",
      "global_step: 550, epoch: 0, batch: 549, loss: 0.01049, precission: 0.88350, recall: 0.91531, f1_score: 0.89913, speed: 4.82 step/s\n",
      "global_step: 560, epoch: 0, batch: 559, loss: 0.03427, precission: 0.88108, recall: 0.91469, f1_score: 0.89757, speed: 4.92 step/s\n",
      "global_step: 570, epoch: 0, batch: 569, loss: 0.03204, precission: 0.88167, recall: 0.91483, f1_score: 0.89795, speed: 4.97 step/s\n",
      "global_step: 580, epoch: 0, batch: 579, loss: 0.04976, precission: 0.88195, recall: 0.91513, f1_score: 0.89824, speed: 5.26 step/s\n",
      "global_step: 590, epoch: 0, batch: 589, loss: 0.01673, precission: 0.88246, recall: 0.91591, f1_score: 0.89887, speed: 4.72 step/s\n",
      "global_step: 600, epoch: 0, batch: 599, loss: 0.01732, precission: 0.88292, recall: 0.91526, f1_score: 0.89880, speed: 4.49 step/s\n",
      "eval dev loss: 0.02568, precission: 0.90797, recall: 0.92122, f1_score: 0.91455\n",
      "save model at global step : 600, best_precission: 0.90797, best_recall: 0.00000, best val f1_score: 0.91455\n",
      "global_step: 610, epoch: 0, batch: 609, loss: 0.01513, precission: 0.90464, recall: 0.91990, f1_score: 0.91221, speed: 0.68 step/s\n",
      "global_step: 620, epoch: 0, batch: 619, loss: 0.02642, precission: 0.90440, recall: 0.92065, f1_score: 0.91245, speed: 4.76 step/s\n",
      "global_step: 630, epoch: 0, batch: 629, loss: 0.03536, precission: 0.90222, recall: 0.91794, f1_score: 0.91001, speed: 4.91 step/s\n",
      "global_step: 640, epoch: 0, batch: 639, loss: 0.01665, precission: 0.90065, recall: 0.91831, f1_score: 0.90939, speed: 4.95 step/s\n",
      "global_step: 650, epoch: 0, batch: 649, loss: 0.01646, precission: 0.90006, recall: 0.91816, f1_score: 0.90902, speed: 4.87 step/s\n",
      "global_step: 660, epoch: 0, batch: 659, loss: 0.02006, precission: 0.89895, recall: 0.91701, f1_score: 0.90789, speed: 4.55 step/s\n",
      "global_step: 670, epoch: 0, batch: 669, loss: 0.07145, precission: 0.89315, recall: 0.91378, f1_score: 0.90335, speed: 4.75 step/s\n",
      "global_step: 680, epoch: 0, batch: 679, loss: 0.04449, precission: 0.88868, recall: 0.91186, f1_score: 0.90012, speed: 4.53 step/s\n",
      "global_step: 690, epoch: 0, batch: 689, loss: 0.01970, precission: 0.88813, recall: 0.91316, f1_score: 0.90047, speed: 4.89 step/s\n",
      "global_step: 700, epoch: 0, batch: 699, loss: 0.03931, precission: 0.88685, recall: 0.91239, f1_score: 0.89944, speed: 5.13 step/s\n",
      "eval dev loss: 0.02720, precission: 0.90079, recall: 0.91916, f1_score: 0.90988\n",
      "global_step: 710, epoch: 0, batch: 709, loss: 0.03386, precission: 0.90107, recall: 0.91958, f1_score: 0.91023, speed: 0.96 step/s\n",
      "global_step: 720, epoch: 0, batch: 719, loss: 0.03840, precission: 0.90057, recall: 0.91999, f1_score: 0.91017, speed: 4.51 step/s\n",
      "global_step: 730, epoch: 0, batch: 729, loss: 0.04681, precission: 0.89543, recall: 0.91761, f1_score: 0.90639, speed: 4.68 step/s\n",
      "global_step: 740, epoch: 0, batch: 739, loss: 0.03006, precission: 0.89505, recall: 0.91684, f1_score: 0.90582, speed: 4.67 step/s\n",
      "global_step: 750, epoch: 0, batch: 749, loss: 0.02675, precission: 0.89372, recall: 0.91684, f1_score: 0.90513, speed: 4.75 step/s\n",
      "global_step: 760, epoch: 0, batch: 759, loss: 0.04852, precission: 0.89156, recall: 0.91621, f1_score: 0.90372, speed: 4.62 step/s\n",
      "global_step: 770, epoch: 0, batch: 769, loss: 0.03238, precission: 0.89104, recall: 0.91522, f1_score: 0.90297, speed: 5.15 step/s\n",
      "global_step: 780, epoch: 0, batch: 779, loss: 0.00731, precission: 0.89078, recall: 0.91590, f1_score: 0.90317, speed: 4.94 step/s\n",
      "global_step: 790, epoch: 0, batch: 789, loss: 0.04169, precission: 0.88986, recall: 0.91581, f1_score: 0.90265, speed: 4.78 step/s\n",
      "global_step: 800, epoch: 0, batch: 799, loss: 0.03296, precission: 0.88970, recall: 0.91484, f1_score: 0.90210, speed: 4.66 step/s\n",
      "eval dev loss: 0.02421, precission: 0.91616, recall: 0.92833, f1_score: 0.92220\n",
      "save model at global step : 800, best_precission: 0.91616, best_recall: 0.00000, best val f1_score: 0.92220\n",
      "global_step: 810, epoch: 0, batch: 809, loss: 0.01744, precission: 0.91456, recall: 0.92992, f1_score: 0.92218, speed: 0.67 step/s\n",
      "global_step: 820, epoch: 0, batch: 819, loss: 0.04033, precission: 0.91428, recall: 0.92918, f1_score: 0.92167, speed: 4.60 step/s\n",
      "global_step: 830, epoch: 0, batch: 829, loss: 0.03685, precission: 0.90905, recall: 0.92692, f1_score: 0.91790, speed: 4.80 step/s\n",
      "global_step: 840, epoch: 0, batch: 839, loss: 0.01064, precission: 0.90612, recall: 0.92424, f1_score: 0.91509, speed: 4.90 step/s\n",
      "global_step: 850, epoch: 0, batch: 849, loss: 0.01325, precission: 0.90607, recall: 0.92506, f1_score: 0.91547, speed: 4.78 step/s\n",
      "global_step: 860, epoch: 0, batch: 859, loss: 0.01570, precission: 0.90542, recall: 0.92497, f1_score: 0.91509, speed: 4.74 step/s\n",
      "global_step: 870, epoch: 0, batch: 869, loss: 0.01104, precission: 0.90670, recall: 0.92500, f1_score: 0.91576, speed: 4.83 step/s\n",
      "global_step: 880, epoch: 0, batch: 879, loss: 0.00989, precission: 0.90615, recall: 0.92513, f1_score: 0.91554, speed: 5.18 step/s\n",
      "global_step: 890, epoch: 0, batch: 889, loss: 0.00742, precission: 0.90647, recall: 0.92517, f1_score: 0.91573, speed: 4.62 step/s\n",
      "global_step: 900, epoch: 0, batch: 899, loss: 0.03652, precission: 0.90470, recall: 0.92497, f1_score: 0.91472, speed: 4.86 step/s\n",
      "eval dev loss: 0.02422, precission: 0.92049, recall: 0.92721, f1_score: 0.92384\n",
      "save model at global step : 900, best_precission: 0.92049, best_recall: 0.00000, best val f1_score: 0.92384\n",
      "global_step: 910, epoch: 0, batch: 909, loss: 0.04425, precission: 0.91790, recall: 0.92675, f1_score: 0.92230, speed: 0.68 step/s\n",
      "global_step: 920, epoch: 0, batch: 919, loss: 0.05778, precission: 0.91364, recall: 0.92603, f1_score: 0.91979, speed: 5.19 step/s\n",
      "global_step: 930, epoch: 0, batch: 929, loss: 0.01680, precission: 0.91348, recall: 0.92671, f1_score: 0.92005, speed: 4.97 step/s\n",
      "global_step: 940, epoch: 0, batch: 939, loss: 0.02194, precission: 0.91270, recall: 0.92781, f1_score: 0.92020, speed: 4.53 step/s\n",
      "global_step: 950, epoch: 0, batch: 949, loss: 0.01911, precission: 0.91194, recall: 0.92769, f1_score: 0.91975, speed: 5.01 step/s\n",
      "global_step: 960, epoch: 0, batch: 959, loss: 0.01888, precission: 0.91063, recall: 0.92651, f1_score: 0.91850, speed: 4.84 step/s\n",
      "global_step: 970, epoch: 0, batch: 969, loss: 0.04975, precission: 0.90939, recall: 0.92612, f1_score: 0.91768, speed: 4.99 step/s\n",
      "global_step: 980, epoch: 0, batch: 979, loss: 0.02620, precission: 0.91031, recall: 0.92717, f1_score: 0.91866, speed: 4.95 step/s\n",
      "global_step: 990, epoch: 0, batch: 989, loss: 0.02857, precission: 0.90910, recall: 0.92770, f1_score: 0.91831, speed: 4.92 step/s\n",
      "global_step: 1000, epoch: 0, batch: 999, loss: 0.01133, precission: 0.90776, recall: 0.92631, f1_score: 0.91694, speed: 4.97 step/s\n",
      "eval dev loss: 0.02357, precission: 0.92548, recall: 0.93881, f1_score: 0.93209\n",
      "save model at global step : 1000, best_precission: 0.92548, best_recall: 0.00000, best val f1_score: 0.93209\n",
      "global_step: 1010, epoch: 0, batch: 1009, loss: 0.02469, precission: 0.92183, recall: 0.93701, f1_score: 0.92936, speed: 0.67 step/s\n",
      "global_step: 1020, epoch: 0, batch: 1019, loss: 0.01688, precission: 0.92022, recall: 0.93598, f1_score: 0.92803, speed: 4.62 step/s\n",
      "global_step: 1030, epoch: 0, batch: 1029, loss: 0.02303, precission: 0.92015, recall: 0.93487, f1_score: 0.92745, speed: 4.62 step/s\n",
      "global_step: 1040, epoch: 0, batch: 1039, loss: 0.02230, precission: 0.91757, recall: 0.93417, f1_score: 0.92580, speed: 4.56 step/s\n",
      "global_step: 1050, epoch: 0, batch: 1049, loss: 0.02062, precission: 0.91458, recall: 0.93228, f1_score: 0.92335, speed: 4.81 step/s\n",
      "global_step: 1060, epoch: 0, batch: 1059, loss: 0.00410, precission: 0.91400, recall: 0.93260, f1_score: 0.92321, speed: 5.07 step/s\n",
      "global_step: 1070, epoch: 0, batch: 1069, loss: 0.01427, precission: 0.91189, recall: 0.93090, f1_score: 0.92130, speed: 4.79 step/s\n",
      "global_step: 1080, epoch: 0, batch: 1079, loss: 0.00452, precission: 0.90711, recall: 0.92896, f1_score: 0.91791, speed: 4.99 step/s\n",
      "global_step: 1090, epoch: 0, batch: 1089, loss: 0.03289, precission: 0.90236, recall: 0.92721, f1_score: 0.91462, speed: 4.86 step/s\n",
      "global_step: 1100, epoch: 0, batch: 1099, loss: 0.01755, precission: 0.90259, recall: 0.92691, f1_score: 0.91459, speed: 4.70 step/s\n",
      "eval dev loss: 0.02586, precission: 0.91823, recall: 0.93301, f1_score: 0.92556\n",
      "global_step: 1110, epoch: 0, batch: 1109, loss: 0.00978, precission: 0.91785, recall: 0.93350, f1_score: 0.92561, speed: 0.97 step/s\n",
      "global_step: 1120, epoch: 0, batch: 1119, loss: 0.05316, precission: 0.91446, recall: 0.93195, f1_score: 0.92312, speed: 5.00 step/s\n",
      "global_step: 1130, epoch: 0, batch: 1129, loss: 0.00963, precission: 0.91393, recall: 0.93174, f1_score: 0.92275, speed: 4.48 step/s\n",
      "global_step: 1140, epoch: 0, batch: 1139, loss: 0.04887, precission: 0.91577, recall: 0.93301, f1_score: 0.92431, speed: 4.86 step/s\n",
      "global_step: 1150, epoch: 0, batch: 1149, loss: 0.01770, precission: 0.91537, recall: 0.93337, f1_score: 0.92428, speed: 4.71 step/s\n",
      "global_step: 1160, epoch: 0, batch: 1159, loss: 0.02252, precission: 0.91463, recall: 0.93340, f1_score: 0.92392, speed: 5.16 step/s\n",
      "global_step: 1170, epoch: 0, batch: 1169, loss: 0.01544, precission: 0.91411, recall: 0.93256, f1_score: 0.92325, speed: 4.99 step/s\n",
      "global_step: 1180, epoch: 0, batch: 1179, loss: 0.08800, precission: 0.91394, recall: 0.93235, f1_score: 0.92305, speed: 4.97 step/s\n",
      "global_step: 1190, epoch: 0, batch: 1189, loss: 0.00506, precission: 0.91249, recall: 0.93176, f1_score: 0.92202, speed: 5.06 step/s\n",
      "global_step: 1200, epoch: 0, batch: 1199, loss: 0.03838, precission: 0.91144, recall: 0.93139, f1_score: 0.92131, speed: 4.80 step/s\n",
      "eval dev loss: 0.02329, precission: 0.92173, recall: 0.92552, f1_score: 0.92362\n",
      "global_step: 1210, epoch: 0, batch: 1209, loss: 0.02766, precission: 0.92082, recall: 0.92634, f1_score: 0.92358, speed: 0.96 step/s\n",
      "global_step: 1220, epoch: 0, batch: 1219, loss: 0.02323, precission: 0.91840, recall: 0.92478, f1_score: 0.92158, speed: 4.72 step/s\n",
      "global_step: 1230, epoch: 0, batch: 1229, loss: 0.00480, precission: 0.91397, recall: 0.92160, f1_score: 0.91777, speed: 4.82 step/s\n",
      "global_step: 1240, epoch: 0, batch: 1239, loss: 0.02996, precission: 0.91165, recall: 0.92122, f1_score: 0.91641, speed: 5.19 step/s\n",
      "global_step: 1250, epoch: 0, batch: 1249, loss: 0.02625, precission: 0.91122, recall: 0.92206, f1_score: 0.91661, speed: 5.51 step/s\n",
      "global_step: 1260, epoch: 0, batch: 1259, loss: 0.02329, precission: 0.91021, recall: 0.92224, f1_score: 0.91619, speed: 4.60 step/s\n",
      "global_step: 1270, epoch: 0, batch: 1269, loss: 0.00514, precission: 0.91042, recall: 0.92246, f1_score: 0.91640, speed: 5.11 step/s\n",
      "global_step: 1280, epoch: 0, batch: 1279, loss: 0.01940, precission: 0.91011, recall: 0.92313, f1_score: 0.91657, speed: 4.88 step/s\n",
      "global_step: 1290, epoch: 0, batch: 1289, loss: 0.01445, precission: 0.90962, recall: 0.92387, f1_score: 0.91669, speed: 4.90 step/s\n",
      "global_step: 1300, epoch: 0, batch: 1299, loss: 0.01559, precission: 0.90998, recall: 0.92454, f1_score: 0.91720, speed: 4.78 step/s\n",
      "eval dev loss: 0.02403, precission: 0.92844, recall: 0.92983, f1_score: 0.92913\n",
      "global_step: 1310, epoch: 0, batch: 1309, loss: 0.03262, precission: 0.92818, recall: 0.93102, f1_score: 0.92960, speed: 0.97 step/s\n",
      "global_step: 1320, epoch: 0, batch: 1319, loss: 0.02912, precission: 0.92554, recall: 0.93089, f1_score: 0.92821, speed: 4.69 step/s\n",
      "global_step: 1330, epoch: 0, batch: 1329, loss: 0.02594, precission: 0.92474, recall: 0.93146, f1_score: 0.92809, speed: 4.79 step/s\n",
      "global_step: 1340, epoch: 0, batch: 1339, loss: 0.00616, precission: 0.92384, recall: 0.93157, f1_score: 0.92769, speed: 4.90 step/s\n",
      "global_step: 1350, epoch: 0, batch: 1349, loss: 0.01539, precission: 0.92167, recall: 0.93025, f1_score: 0.92594, speed: 4.64 step/s\n",
      "global_step: 1360, epoch: 0, batch: 1359, loss: 0.02270, precission: 0.91815, recall: 0.93002, f1_score: 0.92405, speed: 5.13 step/s\n",
      "global_step: 1370, epoch: 0, batch: 1369, loss: 0.01664, precission: 0.91621, recall: 0.92950, f1_score: 0.92281, speed: 5.25 step/s\n",
      "global_step: 1380, epoch: 0, batch: 1379, loss: 0.00815, precission: 0.91680, recall: 0.92976, f1_score: 0.92323, speed: 4.89 step/s\n",
      "global_step: 1390, epoch: 0, batch: 1389, loss: 0.01064, precission: 0.91370, recall: 0.92892, f1_score: 0.92124, speed: 4.77 step/s\n",
      "global_step: 1400, epoch: 0, batch: 1399, loss: 0.02534, precission: 0.91385, recall: 0.92911, f1_score: 0.92142, speed: 4.61 step/s\n",
      "eval dev loss: 0.02397, precission: 0.91615, recall: 0.93432, f1_score: 0.92514\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=ignore_label)\r\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=False)\r\n",
    "optimizer = AdamW(learning_rate=2e-5, parameters=model.parameters())\r\n",
    "\r\n",
    "global_step = 0\r\n",
    "epochs = 1\r\n",
    "best_f1_score = 0.0\r\n",
    "best_precission = 0.0\r\n",
    "best_recall = 0.0\r\n",
    "\r\n",
    "print_every_step = 10\r\n",
    "evaluate_every_step = 100\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "tic_train = time.time()\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    for step, batch_data in enumerate(train_loader):\r\n",
    "        \r\n",
    "        input_ids, token_type_ids, length, labels = batch_data\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "\r\n",
    "        loss = loss_fn(logits, labels)\r\n",
    "\r\n",
    "        preds = paddle.argmax(logits, axis=-1)\r\n",
    "        n_infer, n_label, n_correct = metric.compute(None, length, preds, labels)\r\n",
    "        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\r\n",
    "        precission, recall, f1_score = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "\r\n",
    "        if global_step % print_every_step == 0:\r\n",
    "            print('global_step: %d, epoch: %d, batch: %d, loss: %.5f, precission: %.5f, recall: %.5f, f1_score: %.5f, speed: %.2f step/s' % (\r\n",
    "                global_step, epoch, step, loss.numpy(), precission, recall, f1_score, print_every_step / (time.time() - tic_train)\r\n",
    "            ))\r\n",
    "            tic_train = time.time()\r\n",
    "        \r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % evaluate_every_step == 0:\r\n",
    "            loss, precission, recall, f1_score = evaluate(model, loss_fn, metric, dev_loader)\r\n",
    "            print('eval dev loss: %.5f, precission: %.5f, recall: %.5f, f1_score: %.5f' % (\r\n",
    "                loss, precission, recall, f1_score\r\n",
    "            ))\r\n",
    "            if f1_score > best_f1_score and precission > best_precission and recall > best_recall:\r\n",
    "                \r\n",
    "                best_f1_score = f1_score\r\n",
    "                best_precission = precission \r\n",
    "                best_recall = best_recall\r\n",
    "\r\n",
    "                print('save model at global step : %d, best_precission: %.5f, best_recall: %.5f, best val f1_score: %.5f' % (\r\n",
    "                    global_step, best_precission, best_recall, best_f1_score\r\n",
    "                ))\r\n",
    "\r\n",
    "                paddle.save(model.state_dict(), './bert_result/model_%d.pdparams' % step)\r\n",
    "\r\n",
    "\r\n",
    "# step = 0\r\n",
    "# for epoch in range(10):\r\n",
    "#     for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\r\n",
    "#         logits = model(input_ids, token_type_ids)\r\n",
    "#         loss = paddle.mean(loss_fn(logits, labels))\r\n",
    "#         loss.backward()\r\n",
    "#         optimizer.step()\r\n",
    "#         optimizer.clear_grad()\r\n",
    "#         step += 1\r\n",
    "#         print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\r\n",
    "#     evaluate(model, metric, dev_loader)\r\n",
    "\r\n",
    "#     paddle.save(model.state_dict(),\r\n",
    "#                 './bert_result/model_%d.pdparams' % step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: bert_result.txt, some examples are shown below: \n",
      "('中共中央', 'O')('致中国致公党十一大', 'O')('的', 'O')('贺', 'O')('词', 'O')('各', 'O')('位', 'O')('代', 'O')('表', 'O')('、', 'O')('各', 'O')('位', 'O')('同', 'O')('志', 'O')('：', 'O')('在中国致公党第十', 'O')('一次全国代表大会', 'O')('隆', 'O')('重', 'O')('召', 'O')('开', 'O')('之', 'O')('际', 'O')('，中国共产党中央委员会', 'O')('谨', 'O')('向', 'O')('大', 'O')('会', 'O')('表', 'O')('示', 'O')('热', 'O')('烈', 'O')('的', 'O')('祝', 'O')('贺', 'O')('，', 'O')('向致公党', 'O')('的', 'O')('同', 'O')('志', 'O')('们', 'O')('致', 'O')('以', 'O')('亲', 'O')('切', 'O')('的', 'O')('问', 'O')('候', 'O')\n",
      "('在', 'O')('过', 'O')('去', 'O')('的', 'O')('五', 'O')('年', 'O')('中', 'O')('，致公党', 'O')('在邓小平', 'O')('理', 'O')('论', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('遵', 'O')('循', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('初', 'O')('级', 'O')('阶', 'O')('段', 'O')('的', 'O')('基', 'O')('本', 'O')('路', 'O')('线', 'O')('，', 'O')('努', 'O')('力', 'O')('实', 'O')('践致公党十大', 'O')('提', 'O')('出', 'O')('的', 'O')('发', 'O')('挥', 'O')('参', 'O')('政', 'O')('党', 'O')('职', 'O')('能', 'O')('、', 'O')('加', 'O')('强', 'O')('自', 'O')('身', 'O')('建', 'O')('设', 'O')('的', 'O')('基', 'O')('本', 'O')('任', 'O')('务', 'O')('。', 'O')\n",
      "('高', 'O')('举', 'O')('爱', 'O')('国', 'O')('主', 'O')('义', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('两', 'O')('面', 'O')('旗', 'O')('帜', 'O')('，', 'O')('团', 'O')('结', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('以', 'O')('及', 'O')('所', 'O')('联', 'O')('系', 'O')('的', 'O')('归', 'O')('侨', 'O')('、', 'O')('侨', 'O')('眷', 'O')('，', 'O')('发', 'O')('扬', 'O')('爱', 'O')('国', 'O')('革', 'O')('命', 'O')('的', 'O')('光', 'O')('荣', 'O')('传', 'O')('统', 'O')('，', 'O')('为', 'O')('统', 'O')('一', 'O')('祖', 'O')('国', 'O')('、', 'O')('振', 'O')('兴中华', 'O')('而', 'O')('努', 'O')('力', 'O')('奋', 'O')('斗', 'O')('；', 'O')('紧', 'O')('紧', 'O')('围', 'O')('绕', 'O')('国', 'O')('家', 'O')('的', 'O')('中', 'O')('心', 'O')('工', 'O')('作', 'O')('，', 'O')('联', 'O')('系', 'O')('改', 'O')('革', 'O')('和', 'O')('建', 'O')('设', 'O')('中', 'O')('的', 'O')('重', 'O')('大', 'O')('问', 'O')('题', 'O')('以', 'O')('及', 'O')('人', 'O')('民', 'O')('群', 'O')('众', 'O')('普', 'O')('遍', 'O')('关', 'O')('心', 'O')('的', 'O')('社', 'O')('会', 'O')('问', 'O')('题', 'O')('，', 'O')('深', 'O')('入', 'O')('开', 'O')('展', 'O')('调', 'O')('查', 'O')('研', 'O')('究', 'O')('，', 'O')('就', 'O')('经', 'O')('济', 'O')('建', 'O')('设', 'O')('、', 'O')('侨', 'O')('务', 'O')('政', 'O')('策', 'O')('、', 'O')('文', 'O')('教', 'O')('卫', 'O')('生', 'O')\n",
      "('在', 'O')('此', 'O')('，中共中央', 'O')('谨', 'O')('向致公党中央', 'O')('以', 'O')('及', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('致', 'O')('以', 'O')('崇', 'O')('高', 'O')('的', 'O')('敬', 'O')('意', 'O')('！', 'O')\n",
      "('不', 'O')('久', 'O')('前', 'O')('，中国共产党', 'O')('召', 'O')('开', 'O')('了', 'O')('举', 'O')('世', 'O')('瞩', 'O')('目', 'O')('的', 'O')('第', 'O')('十', 'O')('五', 'O')('次', 'O')('全', 'O')('国', 'O')('代', 'O')('表', 'O')('大', 'O')('会', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('代', 'O')('表', 'O')('大', 'O')('会', 'O')('是', 'O')('在中国', 'O')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('发', 'O')('展', 'O')('的', 'O')('关', 'O')('键', 'O')('时', 'O')('刻', 'O')('召', 'O')('开', 'O')('的', 'O')('历', 'O')('史', 'O')('性', 'O')('会', 'O')('议', 'O')('。', 'O')\n",
      "('大', 'O')('会', 'O')('高', 'O')('举邓小平', 'O')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('回', 'O')('顾', 'O')('一', 'O')('个', 'O')('世', 'O')('纪', 'O')('以', 'O')('来中国', 'O')('人', 'O')('民', 'O')('的', 'O')('奋', 'O')('斗', 'O')('历', 'O')('史', 'O')('，', 'O')('展', 'O')('望', 'O')('下', 'O')('个', 'O')('世', 'O')('纪', 'O')('５', 'O')('０', 'O')('年', 'O')('的', 'O')('发', 'O')('展', 'O')('前', 'O')('景', 'O')('，', 'O')('认', 'O')('真', 'O')('总', 'O')('结', 'O')('了中共', 'O')('十', 'O')('一', 'O')('届', 'O')('三', 'O')('中', 'O')('全', 'O')('会', 'O')('以', 'O')('来', 'O')('特', 'O')('别', 'O')('是十四大', 'O')('以', 'O')('来', 'O')('的', 'O')('实', 'O')('践', 'O')('经', 'O')('验', 'O')('，', 'O')('对中国', 'O')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('跨', 'O')('世', 'O')('纪', 'O')('的', 'O')('发', 'O')('展', 'O')('作', 'O')('出', 'O')('了', 'O')('全', 'O')('面', 'O')('部', 'O')('署', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('大', 'O')('会', 'O')('对', 'O')('于', 'O')('动', 'O')('员', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('，', 'O')('解', 'O')('放', 'O')('思', 'O')('想', 'O')('，', 'O')('实', 'O')('事', 'O')('求', 'O')('是', 'O')('，', 'O')('抓', 'O')('住', 'O')('有', 'O')('利', 'O')('时', 'O')('机', 'O')('，', 'O')('继', 'O')('续', 'O')('开', 'O')('拓', 'O')('前', 'O')('进', 'O')('，', 'O')('把', 'O')('建', 'O')('设', 'O')('有中国', 'O')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('伟', 'O')('大', 'O')('事', 'O')('业', 'O')('全', 'O')('面', 'O')('推', 'O')('向', 'O')('２', 'O')('１', 'O')('世', 'O')('纪', 'O')('，', 'O')('具', 'O')('有', 'O')('极', 'O')('其', 'O')('重', 'O')('大', 'O')('和', 'O')('深', 'O')('远', 'O')('的', 'O')('意', 'O')('义', 'O')('。', 'O')\n",
      "('当', 'O')('前', 'O')('，', 'O')('在中共十五大', 'O')('精', 'O')('神', 'O')('的', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('在', 'O')('以江泽民', 'O')('同', 'O')('志', 'O')('为', 'O')('核', 'O')('心', 'O')('的中共中央', 'O')('领', 'O')('导', 'O')('下', 'O')('，', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('正', 'O')('高', 'O')('举邓小平', 'O')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('同', 'O')('心', 'O')('同', 'O')('德', 'O')('，', 'O')('团', 'O')('结', 'O')('奋', 'O')('斗', 'O')('，', 'O')('沿', 'O')('着', 'O')('建', 'O')('设', 'O')('有中国', 'O')('特', 'O')('色', 'O')('的', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('道', 'O')('路', 'O')('阔', 'O')('步', 'O')('前', 'O')('进', 'O')('。', 'O')\n",
      "('实', 'O')('现', 'O')('建', 'O')('设', 'O')('有中国', 'O')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('的', 'O')('宏', 'O')('伟', 'O')('目', 'O')('标', 'O')('，', 'O')('是中国共产党', 'O')('和', 'O')('作', 'O')('为', 'O')('参', 'O')('政', 'O')('党', 'O')('的', 'O')('各', 'O')('民', 'O')('主', 'O')('党', 'O')('派', 'O')('共', 'O')('同', 'O')('肩', 'O')('负', 'O')('的', 'O')('历', 'O')('史', 'O')('使', 'O')('命', 'O')('。', 'O')\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader, test_ds, label_vocab)\r\n",
    "file_path = \"bert_result.txt\"\r\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\r\n",
    "    fout.write(\"\\n\".join(preds))\r\n",
    "# Print some examples\r\n",
    "print(\r\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\r\n",
    "    % file_path)\r\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## github截图：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/135156143bed4538baec6b4f41af4f6d7764ba49b3f342da98497c89ad2ff4ff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入课程QQ交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
